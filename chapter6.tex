%===================================== CHAP 6 =================================

\chapter{Discussion and conclusion}
\label{ch:6}

To wrap up the work, this chapter summarizes what has been done in the project and presents some additional remarks. In section \ref{sec:review} the main observations from the results in chapter \ref{ch:results} will be summarized and discussed. For my master thesis the goal is to expand this work, and some of the ideas for this further work is presented in section \ref{sec:FW}. Section \ref{sec:conclusion} gives a conclusion of the project.  

\section{Reviewing the results}
\label{sec:review}
\subsection{Summary}
By implementing the methods described in section \ref{Method} the potential to study one time-varying synaptic connection was investigated. Three test cases were performed, and the results are presented in chapter \ref{ch:results}. 

In test case 1, a static weight was assumed. It was shown that Newton iterations was a sufficient method to learn this weight. It was also demonstrated how the distribution of maximum likelihood values for the weight becomes narrower as the size of the data set grows. 

In test case 2 it was investigated whether a Bayesian framework and Metropolis-Hastings inference could be used to infer dynamical weights for varying amounts of data, with a weight trajectory for nine time steps. Section \ref{sec:CASE2_r} presents plots of how the error develops over iterations for the different number of spike trains. These show that the error decreases significantly with the size of the data set, and that for 10000 spike trains the weights can be inferred quite precisely. Also, the movement of the weight trajectory through the iterations for the case with 10000 spike trains was illustrated. This showed how the estimated trajectory shifted towards the generative weights over the iterations, and that it ended approximately at the generative weights within 1000 iterations. A plot of the estimated weights after 10000 iterations for 10 spike trains for different prior variances was also presented. From that we observed that with this little amount of data, the method was not able to infer the generative weights for any value of the prior variance. 

Finally, in case 3 the intention was to study a situation that was closer to how the real data is expected to be. The generative weight trajectory was longer and based on the additive STDP rule, and there was one spike train only. It was shown that the method did not work when trying to infer a weight for every time step. The method was then tried with constraining the weights to be constant within some equally sized windows. Different prior values and different length of the windows were tried and compared. The resulting figures showed that for a proper window size and prior value, the essential trend in the weight trajectory could be captured with one spike train.

\subsection{Discussion} 

The main findings in the results is that, provided enough data, it is possible to infer the strength of a time varying synaptic connection from spike train data. Since some learning trends could be inferred with only one spike train for the weights generated by the STDP rule, this gives good indications that learning trends will also be possible to capture in real data. 

Another important observation is that, with this little amount of information contained in the prior, few data points gave bad results. A single weight can for example not be inferred with 1 or 10 data points, with the smoothness prior. This was however not surprising. As indicated in the histograms from case 1, even with 100 data points for a single weight it is not unlikely that the maximum likelihood value for the weight as given by the data is above 0.9 or below 0.5, even though the data was generated with a weight value of 0.7. Therefore, one can not expect to for example detect a weight change from 0.7 to 0.8 from only 100 data points. For a smoothness prior, the best we can hope for is that it stabilizes around the mean of the generative weights.

Another problem with the method is that since all weights are drawn independently but simultaneously, the probability of drawing a weight trajectory that is considerably better than the previously accepted one gets very little when the trajectory is long. Lets say that we for example hope to draw a trajectory where least $\frac{3}{4}$ of the weights are moving towards the generative ones compared to the previous trajectory sample. Define success as the event that one weight is drawn to a better position than in the previous iteration. This gives a success rate of 0.5 for a symmetric proposal distribution. Then, we can consider drawing the whole trajectory as one binomial experiment. If we then have 400 time steps, and hope for at least 300 successes, the probability that a such trajectory is drawn from the proposal distribution is of order $10^{-24}$. Even if we only want at least 225 successes, the probability is still less than one percent. Therefore, the result will either be to accept only a extremely small fraction of the draws, or to accept trajectories that doesn't lead us closer to the generative weights. This is the problem in figure \ref{fig:400_weights}.

\section{Further work}
\label{sec:FW}

Even though the findings in this work gives insight for studying neural activity with dynamical connectivity, there was done some considerable simplifications. Therefore, before integrating it with real data, some changes will be made. In this project only activity one time step back in time for the presynaptic neuron was considered to calculate the rate of the postsynaptic neuron. This was not problematic for the inference here, as also the spike activity was simulated this way. For real data however, it is likely that also activity further back in time will have an effect, although the impact probably will decrease with a bigger time lag. Also, previous spiking of the neuron itself, not only of connected neurons will be relevant to consider. 

One main goal later will be to characterize the learning rule from the spike data, rather than the weight trajectory. The assumption that the connections change according to specified learning rules also provides additional information, that can be integrated in the prior. The particle MCMC method, as used in \cite{Linderman}, will be implemented, and the learning rules will be included as prior knowledge in this. This method will also make it possible to infer a longer weight trajectory for individual weights, as now one weight will be drawn at the time. 


\section{Conclusion}
\label{sec:conclusion}

The aim of this work was to develop a method for studying synaptic plasticity by using statistical tools. The motivation for this research topic was that synaptic plasticity is an important function for memory, so knowledge in this field can be a useful contribution to studying memory related diseases. 

In this project a system of two neurons and a single synapse was considered. The activity of the postsynaptic neuron was modeled by a Bernoulli Generalized Linear Model, with a time-varying spike rate that was related to previous activity of the presynaptic neuron through a linear predictor and a logit link function. The connection strength was represented by a time dependent weight. A Bayesian approach with a smoothness prior was taken, and the Metropolis-Hastings algorithm was implemented for inferring the weight trajectory. 

As expected, with little data, it method did not work well. This either resulted in a situation where the MCMC algorithm accepted fluctuating and unstable weight trajectories (for a big prior variance) or where only a straight line without dynamics could be accepted (for a small prior variance). In both cases the generative weights could not be inferred. On the other hand, it was shown that provided enough spike data, the dynamical weights could be inferred with this method. This was demonstrated both by using multiple trials of data for a short weight trajectory, and by using a single spike train for a long weight trajectory with a strict constraint on the weights. This suggests that it can be possible to characterize the learning rules in real data.



%\begin{itemize}
    %\item Short summary of the things that has been done in the project; Neurons form connections to each other, and these connections can vary with time. Various learning rules are hypothesized to govern these dynamics. The strength of connections is reflected by the spiking of neurons. Aim to develop a model to infer time varying connectivity based on spike train data. Spiking is modeled with Bernoulli GLMs. Metropolis-Hastings with smoothness prior used for inference. The method showed to work for enough data material. And as expected it did not work well for little data material per inferred weight. To avoid a solution that jumps around, the prior has to be strict for little data. A strict smoothness prior results in a straight line that cannot learn any weight dynamics. Findings in case 3 suggest that if the weights are changing slowly, and recordings are made for a long time, trends in weight dynamics can be captured with only one spike train.
    %\item The next goal is to infer parameters for the learning rules, rather than the weights. Also, particle MCMC will be used. This includes using the learning rules for the prior... 
%\end{itemize}



\cleardoublepage