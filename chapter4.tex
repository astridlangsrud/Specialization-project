%===================================== CHAP 4 =================================

\chapter{Method and calculations}

Simulated experiments with two neurons and one directed synaptic connection was performed for test purposes. Various approaches for inferring time varying weighs was tried and their limitations were investigated.  the ability of the method to infer time varying weights and to investigate its limitations. In section \ref{Method} the method of the test experiments is described in detail. Section \ref{Precalc} presents some calculations based on probability theory to show how well we can expect the method to perform.

\section{Model}
\label{set_up}

\subsection{Framework}
For the later calculations it is practical to represent the spike data in another format. Define

\begin{equation}
    t \in \{t_1, t_2, ..., t_n\} 
\end{equation}

 where $\{t_1, t_2, ..., t_n\}$ is \ a sequence of equally spaced time steps in [0,T], where $t_{k}-t_{k-1} = t_1$. Also let $s_{i,t} \in \{0,1\}$ be a variable taking value 1 if neuron $i$ spikes in the time bin $(t_{k-1}, t_{k})$, and 0 otherwise. To keep as much information as possible, it is preferable to set the time intervals so small that there is very low probability that the neuron will spike more than once in the intervals. Then, at time $t$ we have a collection of $N$ sequences

\begin{equation}
    S_{t} = \{\{s_{i,t_1}, ,...,s_{i,t-1},s_{it}\}\}_{i=0}^{N} \quad s_{i,t_k} \in {0,1}
\end{equation}

This is illustrated in the following figure, where the top array is a spike train, and the bottom array is the corresponding binary values for the time bins

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{Sp_t.png}
\end{figure} 

The neurons are assumed to be connected in a network. These connections can be represented by a $N \times N$ matrix, $W$, where each element $w_{ij}=w_{ij}$ is a weight corresponding to the strength of the connection between neuron $i$ and neuron $j$. A positive value for $w_{ij}$ corresponds to an excitatory synaptic connection, whereas a negative weight represents an inhibitory. A weight with value zero, means that there is no connection. 
Typically these weights are considered as fixed. However, as the aim is here to study the synaptic plasticity, the weights are let to vary with time. Thus, there is a weight matrix for every time point, $\{W_t\}_{t=0}^{T}$. For a network consisting of three neurons, the weight matrix at time $t$ will look like

\begin{equation*}
W_t = 
\begin{pmatrix}
w_{11,t} & w_{12,t} & w_{13,t}\\
w_{21,t} & w_{22,t} & w_{23,t}\\
w_{31,t} & w_{32,t} & w_{33,t}\\
\end{pmatrix} 
\end{equation*}
\\

\subsection{GLM and MCMC for this case...}
This section will give an outline of how the Bernoulli GLM and MCMC sampling can be used to make inference on the synaptic weights of neurons in a network. The variable $s_i(t)$ for a neuron $i$ at time $t$, as defined in section ??, can be considered as a stochastic variable from a Bernoulli process. The probability parameter, $\pi_{it}$, represents the probability that neuron $i$ will fire an action potential in time bin $(t,t+1)$. The linear predictor, $\eta_{it}$ consists of the spike history of the neurons, scaled by the synaptic weights from them to neuron $i$, in addition to a background firing rat for neuron $i$. As for a Bernoulli GLM the parameter and the linear predictor is connected through a logit link. This can be expressed mathematically as

\begin{equation}
\begin{split}
    P(s_{it}|S_{t-1}, W_t) =  Ber(\pi_{it}) = \pi_{it}^{s_it}(1-\pi_{it})^{1-s_{it}}, \\ \pi_{it} = h(\eta_{it})= \frac{exp(\eta_{it})}{1+exp(\eta_{it})},\\
    \eta_{it} = \Big (\sum_{j=0}^{N}\sum_{t'=0}^{t'=t-1} w_{ji,t'}s_{jt'} \Big) + b_i, \\
    P(S_t|W_t) = \prod_{i,t} P(s_{it}|S_{t-1}, W_t).
\end{split}
\end{equation}

This corresponds to the likelihood of the observed spiking, given some weights. The prior will be dependent on the learning rule later, but for this projects it is set to be a restriction on how much a weight is allowed to vary from one time step to another, namely

\begin{equation}
    \Gamma = \sum_{j=0}^{N} \sum_{t=0}^{T-1} (w_{ji,t+1}-w_{ji,t})^2.
\end{equation}

The prior distribution is a Gauss distribution of $\Gamma$ with zero mean and some variance $\sigma$. Hence, we have the following expression for the posterior distribution for the weights

\begin{equation}
\label{Posterior}
        P(\{W_t\}_{t=1}^{T}|S_t) = \frac{P(S_t|W_t)\times P(W_t)}{P(S_t)} = \frac{\Big\{\prod_{i,t} P(s_{it}|S_{t-1}, W_t)\Big\} \times \Big\{\frac{exp(-\Gamma /2\sigma)}{\sqrt{2\pi \sigma}}\Big\}}{P(S_t)}, 
\end{equation}

\begin{equation}
        P(S_t) = \int \Big\{\prod_{i,t} P(s_{it}|S_{t-1}, W_t)\Big\} \times \Big\{\frac{exp(-\Gamma /2\sigma)}{\sqrt{2\pi \sigma}}\Big\} d{W_t}.
\end{equation}

Inference will be made by the use of MCMC, so the interesting value to compute is the fraction of the posterior distribution given some suggested new value for the weights, $W_{t, new}$ and that of the previously accepted value $W_t$. Hence, the denominator $P(S_t)$ will disappear, and there is no need to compute it. The product of factors less then one will decrease towards zero as the number of factors increases. Therefore, to avoid computer rounding to zero of the likelihood for a large number of time steps, it is necessary to use the log values instead. The value we are computing is hence the log of equation ??  inserting the posterior distribution in equation \ref{Posterior}. The expression is the following

\begin{equation}
\label{eq:ratio}
\begin{split}
    log \frac{\big \{ \prod_{i,t} P(s_{it}|S_{t-1}, W_{t,new})\big \} \times exp(-\Gamma_{new} /2\sigma)}{\big \{ \prod_{i,t}  P(s_{it}|S_{t-1}, W_t) \big \} \times exp(-\Gamma /2\sigma)} \\
    = \Big \{ \sum_{i,t} log( P(s_{it}|S_{t-1}, W_{t,new})) - log( P(s_{it}|S_{t-1}, W_t)) \Big \} -\Gamma_{new} /2\sigma +  \Gamma /2\sigma
\end{split}
\end{equation}

For the MCMC procedure it will be set some initial guess for the weight trajectory $W_t^0$. For the test cases in this project, this will be done in two ways. In some cases the initial guess will be set equal to the actual weights used for the spike simulations. This will be to investigate whether the method is able to stay at the correct solution. In other cases the initial weight for the first time point $W_{t_1}^0$ will be specified and the rest of the initial weight trajectory will be drawn one and one from a normal distribution with mean equal to the previous time point. The proposal distribution will be a  multivariate normal distribution with mean vector equal to the weight trajectory from the last iteration and no covariances between them. 


\section{Method for test cases}
\label{Method}

In this project four test cases was performed on a simple system consisting of two neurons, 1 and 2, and a synaptic weight, $w_{12}$ directed from neuron 1 to neuron 2.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{Two_neurons_illustration.png}
\end{figure}

$N_1$ has for each time step a constant probability for spiking that depends on some background rate $b_1$. $N_2$ has spiking probability that depends on the spiking of $N_1$ in the previous time step through the linear predictor $b_2 + w_{21,t} \cdot s_{1,t-1}$. 

\begin{equation}
\begin{split}
\label{Eq:TestCase}
    s_{1t} \sim \text{Ber}(\pi_{1t}) \hspace{3cm} \pi_{1t}= \frac{1}{1+e^{-b_1}} \\
    s_{2t} \sim \text{Ber}(\pi_{2t}) \hspace{1cm} \pi_{2t}= \frac{1}{1+e^{-(b_2 + w_{21,t} \cdot s_{1,t-1})}}
\end{split}
\end{equation}

For the two first test cases the parameter values to be used are $b1 = 0.5$ and $b2 = 0$. This corresponds to a background firing rate of 0.62 and 0.5 for neuron 1 and 2 respectively. The value for the weight $w_{12}$ will be let to vary with time, but a typical size for it will be $w_{12}=0.7$. This gives a value of 1.2 for the linear predictor when neuron 1 spiked in the previous time step, and a corresponding firing rate of 0.77 for neuron 2. This is summarized in table \ref{table:parameters}.


\begin{table}[!h]
\centering
\begin{tabular}{|c|c|c|c|}
	\hline
	Neuron & b & \pi_{s_{1,t-1}=0} & \pi_{s_{1,t-1}=1} \\
	\hline\hline
	1 & 0.5 & 0.62 & 0.62\\
	\hline
	2 & 0  & 0.5 & 0.77\\
	\hline
\end{tabular}
\caption{Background parameter and spike rates for neuron 1 and neuron 2}
\label{table:parameters}
\end{table}

\subsection{CASE 1: Inferring constant weight with Newton method}

The series of Bernoulli variables $\{s_{1t}\}_{t=1}^T$ and $\{s_{2t}\}_{t=1}^T$ were drawn according to equations \ref{Eq:TestCase}, with constant weight $w_{21,t} = w_{21} = 0.7$ and $T=1000$. The score and observed Fisher value for this example was calculated as 

\begin{equation}
\begin{split}
    s(w_{12}) = \sum_{t=2}^{T} s_{1, t-1} (s_{2,t}-\pi_{2,t}), \\
    H(w_{12}) = \sum_{t=2}^T s_{1, t-1}^2 \pi_t(1-\pi_t) = \sum_{t=1}^T s_{1, t-1} \pi_t(1-\pi_t).
\end{split}
\end{equation}

The last equivalence works since $s_{1,t}$ only can take values 1 or 0. The initial guess was set to $w_{12} = 1$. Newton iterations, as given by equation \ref{eq:Newton}, were performed until convergence.\\ 

\subsection{CASE 2: Inferring dynamic weights with Metropolis-Hastings}
Now the weights were let to vary, and the aim was to infer the vector $\{w_{12,t}\}_{t=1}^T$ of weights for each time step. In this test case 10 time steps was used, namely $T=10$. The weight corresponding to $t = 1$ is not interesting, as no spikes are generated by it, so there are 9 weights to infer.

To begin with a "true" weight trajectory to be used for the spike simulations was specified. The weight at $t=1$ was set to 0.7, and the remaining weights were drawn as

\begin{equation}
    w_{t} = w_{t-1} + |n(0,0.001)|,
\end{equation}

where $|n(0,0.001)|$ is the absolute value of a normal distributed parameter with zero mean and variance equal to $0.001$. In reality the number of time steps is much larger, and changing much slower. However, it is chosen this way to easily gain insight into the issue of inferring varying weights. Spike trains for the two neurons were then drawn using the weight trajectory. These spike trains were drawn various number of times for the same weight trajectory, for comparing the goodness of the algorithm for different data sizes. In other words, for one weight trajectory $\{w_{12,t}\}_{t=1}^T$ it was simulated spikes
$\{\{s_{1,i,t}\}_{t=1}^T\}_{i=1}^I$ and $\{\{s_{2,i,t}\}_{t=1}^T\}_{i=1}^I$ for $I \in \{1,10,100,1000,10000 \}$.

To infer the weights from the data, the Metropolis-Hastings algorithm was used.

The value for the log ratio, as given by equation \ref{eq:ratio}, is now given by

\begin{equation}
    lr = \sum_i \sum_t s_{2,t,i}(b_2 w_{12,t}*s_{1,t-1,i}) - log(1 + exp(b_2 w_{12,t} * s_{1,t-1,i}))
\end{equation}


\\

\subsection{CASE 3: Weights generated by learning rule}


\section{Precalculations}
\label{Precalc}

\begin{itemize}
    \item The aim here is to calculate how well I expect my tests to work. Can use hypothesis testing on bernoulli GLM 
    \item Say something about how many time points ect that is needed to be able to detect a change of the weights
\end{itemize}

\begin{wraptable}{r}{4cm}
\begin{center}
 \begin{tabular}{||c c c ||} 
 \hline
 n & Lower & Upper \\ [0.5ex] 
 \hline\hline
 10 & 0.5 & 1 \\ 
 \hline
 100 & 0.7 & 0.84 \\
 \hline
 1000 & 0.748 & 0.792 \\
 \hline
 10000 & 0.7631 & 0.7769 \\ [1ex] 
 \hline
\end{tabular}
\end{center}
\end{wraptable}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{Conf_intervals.png}
\end{figure}

It is practical to have some understanding of how precisely the weights can be inferred from the spike data. When performing $n$ repeated trials of a Bernoulli process with success rate $\pi$, the sum successes comes from a binomial distribution, with parameters $n$ and $\pi$. For some success count, $n_{success}$, the parameter $\pi$ can be estimated as $\frac{n_{success}}{n}$. How good this estimate is depends on the number of trials, $n$. Figure ?? and table ?? presents the endpoints of range that contains 90\% of the distribution, for different values of n (scaled by one over n). 