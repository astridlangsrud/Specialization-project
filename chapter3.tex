%===================================== CHAP 3 =================================

\chapter{Background and theory}
\label{ch:theory}

\nocite{Regression}


The goal of this project is to develop a model for characterizing the dynamics of synaptic connections based on data material described in section \ref{Lab}. As described in section \ref{NandC}, the synaptic strength is affected by simultaneous spiking of the presynaptic and postsynaptic neuron. Therefore, it is relevant to study the changing of neuronal spike rates in relation to the spiking of the surrounding neurons. A way to achieve this is to model the activity with a Generalized Linear Model. In a short time interval, referred to as a \textit{time step}, the situation of spiking or not spiking for one neuron can be considered as a Bernoulli process, with some time dependent spiking rate $\mu (t)$. This spiking rate can be expressed function of the spike history of the neuron itself and of the other neurons in the network. The relevant statistical theory is described in section \ref{sec:stats}. Section \ref{Intro_GLM} provides a general introduction to the GLM framework and presents how it looks like for a Bernoulli distribution. In section \ref{sec:Inference} it is explained how inference in GLMs can be made according to the maximum likelihood and gradient based methods. The source used for this section is the book \textit{Regression} from Springer. 

For a system of N neurons with stationary connections between all of them, the number of unknown parameters is of $O(N^2)$, whereas the size of the data set is of $O(N^2 \times T)$, where $T$ refers to the number of time steps. For $T$ big enough, this gives a convex problem, which can be examined by using gradient based methods with maximum likelihood. On the contrary, if the connections vary with time, the number of parameters to infer grows to $O(N^2 \times T)$, while the data size remains the same. This means that in absence of additional constraints on the connectivity the system with dynamical weights is severely undersampled and thus maximum likelihood inference as presented in section \ref{sec:Bernoulli} is not feasible. Fortunately the biological system we are modelling in this project, a plastic neuronal network, instructs us on reasonable constraints to the problem, starting from auto-correlations of the synaptic connections and synaptic plasticity rules. It follows then naturally to incorporate this prior knowledge on the model parameters into the inference process by taking a Bayesian approach. A description of the Bayesian framework is given in section \ref{BayesianFW}. The involving of prior distributions typically do not preserve the convexity of the problem, so we have to resort to Monte-Carlo techniques to characterize the posterior. The Metropolis-Hastings algorithm is chosen as a starting point. The background theory for this is provided in section \ref{Metropolis}, where the information is gathered from the source \cite{MC}.

The hypothesized models for connectivity dynamics are based on observations from lab experiments with neural networks. Section \ref{sec:SP} presents background for some general learning rules, those that are investigated in Linderman's paper. In this project the learning rules will only be used for generating neural activity in simulations, and not for inference. For my master thesis inferring dynamical connectivity using learning rules will be a main focus. Therefore, this background is included here to give some context, and to enable a discussion of the results with reference to the learning rules.

\section{Generalized linear models}
\label{sec:stats}

Generalized linear models (GLMs) are a class of regression models extending the linear modelling framework to response variables which are not normally distributed. Maximum likelihood inference on GLMs is often performed by gradient ascent.

\subsection{Introduction}
\label{Intro_GLM}
Consider a system constituted by the variable Y, regarded as response (dependent) variable and a set of variables ${X_j}, {j=1,...,P}$, regarded as explanatory (independent). Let $\{y,\mathbf{x}\}$ be a sample of the system. Then, in linear regression the relationship between the dependent and independent variables is modeled by the linear function

\begin{equation}
\label{eq:general}
    y = \bm{ \beta} \mathbf{x} + \epsilon,
\end{equation}

where $\bm{\beta}$ is a vector of regression coefficients, and $\epsilon$ is some random noise distributed as $N(0, \sigma^{2})$. 

Even though this model is useful for many situations, it has some limitations. For example, if the range of the $x$-values is $(-\infty, \infty)$, letting an $x$ approach infinity while everything else is kept constant makes also the corresponding $y$-value approach infinity (or minus infinity if $\beta$ is negative). Hence, if the range of $y$ should be restricted, the linear model is inappropriate. 

Generalized linear models extends the framework of the general linear models, by allowing the response variable to come from several other distributions than the normal one. The response variable can now be distributed according to some exponential family, which are distributions on the form

\begin{equation}
    f(y;\theta) = \exp\Big(\frac{y \theta - b(\theta)}{\phi} \cdot w + c(y,\phi,w)\Big),
\end{equation}

where $b(\theta)$ and $c(y,\phi,w)$ are known functions, $\theta$ is the canonical parameter, $\phi$ is a nuisance parameter and $w$ is a weight function. The expected value of the distribution, $\mathbf{E}[y] = \mu$ is related to the canonical parameter by,

\begin{equation}
    \mu = b'(\theta).
\end{equation}

An essential property of the GLM framework is that there is a specified functional relationship, $g$, between the linear predictor $\eta = \bf x^T \bm{ \beta}$ and this mean value.

The GLM framework can be summarized by the following three components:

\begin{itemize}

\item Response variable distributed as some exponential family

\begin{equation}
    y \sim f(y;\theta)
\end{equation}

with expected value, $\mathbf{E}[y] = \mu$.

\item Linear predictor

\begin{equation}
    \eta = \bf x^T \bm{ \beta}
\end{equation}

\item Link function

\begin{equation}
    \eta = g(\mu)
\end{equation}

\end{itemize}

If the link function maps also the mean of the response variable to the canonical parameter,

\begin{equation}
\theta = g(\mu),
\end{equation}

it is referred to as the \textit{canonical link function}. It then follows that $\theta=\eta$. The canonical link function is often chosen, as it comes with some advantageous properties for inferring the parameters of the GLM. So the pdf of a GLM with canonical link function can be written

\begin{equation}
   f(y|\bm{\beta}) =  \exp\Big(\frac{y  {\bf x^T} {\bm{ \beta}} - b( \bf x^T \bm{ \beta})}{\phi} \cdot w + c(y,\phi,w)\Big).
\end{equation}

The linear model, given by equation \ref{eq:general}, is one special case of GLMs. It can be defined in the GLM framework, by specifying $y$ as a normal distributed variable with mean $\mu$, and having the identity link function, which is the canonical link function associated to the normal distribution. That is

\begin{equation}
\begin{split}
y \sim N(\mu, \sigma^2) \\
\mu = \eta = \bf x^T \bm{ \beta}.
\end{split}
\end{equation}

\subsubsection{Bernoulli GLM}
\label{sec:Bernoulli}

In a Bernoulli process the response variable, $y$, takes the value 1 with a probability $\mu$, and 0 with the probability $1-\mu$. The corresponding probability density function is,

\begin{equation}
\begin{split}
\label{eq:B}
    f(y|\mu) = Ber(\mu) = \mu^{y}(1-\mu)^{1-y}\\
    = \text{exp} ( y  \times \text{log}\big(\frac{\mu}{1-\mu}\big) + \text{log}(1-\mu)),
\end{split}
\end{equation}

where the bottom line shows that it corresponds to an exponential family. Given stochastic component set to be Bernoulli, a Bernoulli GLM is defined by the choice of a suitable link function. As explained above, the link function relates the linear predictor $\eta$ with the mean of the response variable y, which for the Bernoulli process in equation \ref{eq:B} is 

\begin{equation}
\mathbf{E} [y] = \sum_{y = 0,1} y \times \mu^{y}(1-\mu)^{1-y} = \mu
\end{equation}

A probability parameter can only take value in $[0,1]$. Thus the inverse of the link function, the \textit{response function}, have to be a mapping from the real line to $[0,1]$. The most common is the \textit{logit} link function, which is the canonical link in this case,

\begin{equation}
    \eta = g(\mu) = ln(\frac{\mu}{1-\mu}) \Leftrightarrow 
    \mu = \frac{\exp(\eta)}{1+\exp(\eta)}
\end{equation}


\subsection{Maximum likelihood inference via gradient decent}
\label{sec:Inference}

Given $n$ samples of the explanatory and response variables $\{(y_i,\mathbf{x}_i)\}_{i=1,...,n}$, the linear predictor coefficients $\bm {\beta}$ can be estimated by maximizing the likelihood of the data

\begin{equation}
    L(\bm{\beta}) = \prod_{i=1}^{n} f(y_i |\bm{ \beta})
\end{equation}

For a Bernoulli GLM this is,

\begin{equation}
    L(\bm{\beta}) = \prod_{i=1}^{n} \mu_i(\bm{\beta})^{y_i}(1-\mu_i(\bm{\beta}))^{1-y_i}
\end{equation}

It is often convenient to work with the logarithm of the likelihood, which is maximized by the same $\bm{\beta}$-values as the likelihood. The loglikelihood for a Bernoulli GLM is

\begin{equation}
\begin{split}
    l(\bm{\beta}) = \log \prod_{i=1}^{n} (\mu_i(\bm{\beta})^{y_i}(1-\mu_i(\bm{\beta}))^{1-y_i})  =
    \sum_{i=1}^n log (\mu_i(\bm{\beta})^{y_i}(1-\mu_i(\bm{\beta}))^{1-y_i})\\ =  \sum_{i=1}^{n} y_i \log(\frac{\mu_i(\bm{\beta})}{1-\mu_i(\bm{\beta})}) + \log(1-\mu_i(\bm{\beta}))
\end{split}
\end{equation}

Then, for the canonical link function, $\mu_i(\bm{\beta}) = \frac{\exp(\eta_i)}{1+\exp(\eta_i)} = \frac{\exp({\bf x_i^T} \bm{ \beta})}{1+\exp({\bf x_i^T} \bm{ \beta})}$, one arrives at

\begin{equation}
    l(\bm{\beta}) = \sum_{i=1}^{n} y_i \eta_i - \log(1 + \exp(\eta_i) = \sum_{i=1}^n y_i{\bf x_i^T} \bm{ \beta} - ln(1 + \exp(\bf x_i^T \bm{ \beta}))
\end{equation}

The goal is to find the parameters that maximizes the likelihood. For a convex problem, which is always the case for a GLM with a canonical link function, inference can be done using gradient based iterative methods. The idea of such optimization algorithms is to search in the parameter space in the direction of negative gradient to arrive at a mimima. Or equivalently, for a concave function one searches in the positive direction for the maximum. One famous such method is the Newton method. Generally, for a function $f(x)$ to be maximized in the variable x, one starts by choosing some initial guess $x^{(0)}$, and hence update the approximation in every iteration by

\begin{equation}
    x^{(i+1)} = x^{(i)} - \frac{f'(x^{(i)})}{f''(x^{(i)})}
\end{equation}

It's easy to see that the value of x at which this algorithm converges satisfies $f'(x)=0$. The Newton method for maximum likelihood inference then entails computing the first and second derivatives of the loglikelihood called respectively score function and observed Fisher Information.

The score function is the vector of partial derivatives of the loglikelihood. In the Bernoulli case the score function can be derived as follows

\begin{equation}
\label{eq:score}
\text{score}(\bm{\beta}) = \sum_{i=1}^{n}s_i(\bm{\beta}) = \sum_{i=1}^{n} \frac{\partial l_i(\bm{\beta})}{\partial \bm{\beta}} = \sum_{i=1}^{n} {\bf x_i} \Big ( y_i - \frac{\exp({\bf x_i^T} \bm{ \beta})}{1 + \exp({\bf x_i^T} \bm{ \beta})}\Big )
\end{equation}

The observed Fisher information matrix is defined as 

\begin{equation}
    H(\bm {\beta}) = - \frac{\partial^2 l(\bm {\beta)}}{\partial \bm{\beta} \partial \bm{\beta}^T} = -\frac{\partial s(\bm{\beta})}{\partial \bm{\beta}^T},
\end{equation}

which for the Bernoulli case corresponds to

\begin{equation}
    H(\bm{\beta}) = \sum_{i=1}^{n} {\bf x_i x_i^T} \pi_i(\bm{\beta}) (1-\pi_i(\bm{\beta}))
\end{equation}

Hence, the Newton method for estimating $\beta$, is the iteration scheme

\begin{equation}
\label{eq:Newton}
    \bm{\beta}^{(i+1)} = \bm{\beta}^{(i)} + (H(\bm{\beta}^{(i)}))^{-1} s(\bm{\beta}^{(i)})
\end{equation},

where $H(\bm{\beta}^{(i)})^{-1}$ is the matrix inverse of the observed Fisher information matrix.



\section{Bayesian framework}
\label{BayesianFW}

The Bayesian framework allows to go beyond maximum likelihood inference by incorporating prior knowledge on the model parameters through the prior distribution. Given some data sample ${\bf x}$ modelled with the distribution $P(x;\theta)$, the posterior distribution of the parameter given the data can be expressed according to Bayes theorem, as follows.

\begin{equation}
\label{eq:b}
    P(\theta | {\bf x}) = \frac{P({ \bf x}|\theta)P(\theta)}{P({\bf x})}
\end{equation}

$P({\bf x}|\theta)$ is the likelihood that the data would be observed given $\theta$. $P(\theta)$ is the prior distribution of $\theta$, and expresses some knowledge one has of $\theta$ before seeing the data. $P({\bf x})$ is the marginal distribution of the data, and can be expressed as,

\begin{equation}
    P({\bf x}) = \int P({\bf x},\theta)d\theta.
\end{equation}

This is typically too complicated to solve, and can make it hard to sample directly from the posterior distribution. Monte Carlo techniques, introduced in section 3.3, circumvent this problem by exploiting the proportionality of the posterior $P(\theta\vert x)$ to the numerator of equation \ref{eq:b} at fixed data.

The relative amount that the likelihood and the prior contributes to the posterior depends on how wide their distributions are. If the prior knowledge is that the parameter exists in some narrow window, and this prior knowledge is very certain, the prior distribution will be very peaked. This makes the prior more dominating than if its variance was higher. As the size of the data set grows, the relative contribution of the likelihood function typically  increases. 






%In the gradient ascent method one begins with some initial guess $\beta = \beta^0$, and iterates towards the solution by the direction of the gradient of $\beta^i$, and updating the parameters to move in that direction by some step length $\gamma$.

%\begin{equation}
    %\beta^{i+1} = \beta^{i} + \gamma s(\beta)
%\end{equation}

%For a suitable problem this will converge, and eventually the parameters will have a value sufficiently close to that of maximum likelihood.

%Let $\{t_0, t_1, ..., t_n\}$ be a sequence of equally spaced time steps in [0,T] and let $s_i(t_j) \in \{0,1\}$ be the random event taking value 1 if $N_i$ spikes in the time interval $(t_{j-1}, t_{j})$, and 0 otherwise. If the time intervals are so small that the probability of covering more then one spike is negligible, each time interval can be regarded as a Bernoulli process with some probability, $p$, of spiking. This $p$ can be dependent on for instance the spike history of the other neurons, and the weight matrix.  

\section{Inference with MCMC}
\label{Metropolis}

Inference in the Bayesian framework is often performed using Markov Chain Monte Carlo methods (MCMC). MCMC is a class of algorithm for sampling from a probability distribution, which in Bayesian inference is the posterior distribution over the parameters. A Markov chain Monte Carlo (MCMC) techniques utilizes, as the name suggests, a combination of Markov chains with Monte Carlo sampling. A Markov chain is a sequence of events, typically time indexed, that satisfies the Markov property, which says that future events only depends on the present and not on the past. Monte Carlo techniques is the use of random sampling from a distribution to make numerical estimates. However, this requires that we can sample from the distribution, which is not always straightforward. So the idea of MCMC to construct a Markov chain that has a limiting distribution equal to the one we want to sample from, and this way obtain the sample we are looking for. Metropolis-Hastings is one of several MCMCs.

\subsection{Metropolis-Hastings algorithm}

Assume we want to sample from a target distribution $p(x)$, which cannot be sampled from directly. Also assume that $Q(x|x')$ is conditional distribution of $x$ given $x'$ that is possible to draw direct samples from and that satisfies the following two properties

\begin{enumerate}
    \item $Q(x|x') = Q(x'|x)$
    \item $Q(x|x')>0,   \hspace{4mm} \forall \; (x; p(x)>0), \; (x';(p(x')>0)$
\end{enumerate}

Then the Metropolis-Hastings procedure for sampling form $p(x)$ by using $Q$ as proposal distribution is summarized in algorithm \ref{alg:M-H1}.

\newpage

\begin{algorithm}
\caption{}\label{alg:M-H1}
\begin{algorithmic}
\State Set starting value $x^0$
\For {$i=0,1,2,\ldots$}
\State Draw x' from $Q(x'|x^i)$
\State Compute $\alpha = \frac{p(x')}{p(x^i)}$
\State $x^{i+1} = \begin{cases} x' \text{ with probability } min\{ 1, \alpha \}\\ x^i \text{ with probability } 1-min\{ 1, \alpha \}
\end{cases}$
\EndFor
\end{algorithmic}
\end{algorithm}

The resulting sequence $\{  x^0, x^1, x^2,... \}$ is then a Markov chain with transition probability

\begin{equation}
    T(x^{i+1} = x|x^i) = \begin{cases} min \Big \{ 1, \frac{p(x)}{p(x^i)} \Big \} Q(x|x^i) \hspace{1cm} \text{if } x\neq x^i\\
    1-\sum_{x \neq x^i} min \Big \{ 1, \frac{p(x)}{p(x^i)} \Big \} Q(x|x^i) \hspace{1 cm} \text{if } x=x^i.
\end{cases}
\end{equation}

It can be shown that the reversibility condition, 

\begin{equation}
    p(x)T(x'|x) = p(x')T(x|x'),
\end{equation} 

is satisfied, which implies that $p(x)$ is a stationary distribution of the Markov chain. Hence, for some burn in time $n$, we have that $\{x_n, x_{n+1}, x_{n+2},...\}$ is an approximate sample from $p(x)$.

 %This can be done if we cannot sample from a distribution directly. This means that in collecting sample values $\{\theta_0, \theta_1, \theta_2,...\}$ for an unknown parameter $\theta$, the sample $\{\theta_m, \theta_{m+1}, \theta_{m+2},...\}$ will approximately correspond to a random sample from the posterior distribution for $m$ large enough. 

%The Metropolis-Hastings algorithm is initiated by guessing some first value of the parameter, $\theta_0$, for the sample. In every iteration a new value $\theta_{new}$ is drawn from a proposal distribution, $Q(\theta_{new}|\theta_i)$, that is dependent on the previous accepted $\theta$-value for the sample, $\theta_i$. Q can for example be a gaussian distribution with mean value equal to $\theta_i$. This is the Markov chain part of the algorithm. The next step is to determine if $\theta_{new}$ is accepted for the sample or not. This is done by computing the ratio

%\begin{equation}
    %\alpha = 
    %\frac{P(\theta_{new}|{\bm x})}{P(\theta_i|{\bm x})} = 
    %\frac{P({\bm x}|\theta_{new})P(\theta_{new})}{P({\bm x}|\theta_{i})P(\theta_{i})}
%\end{equation}

%If this ratio is greater than or equal to one, the suggestion $\theta_{new}$ is directly accepted. If the ratio is less than one, the suggestion is accepted with a probability equal to the ratio. If $\theta_{new}$ is accepted, we set $\theta_{i+1} = \theta_{new}$, and if not we set $\theta_{i+1} = \theta_i$.




\section{Modelling synaptic plasticity}
\label{sec:SP}

Decades of experimental research have revealed certain relations for how spiking of neurons affects the strength of connectivity between them. There are various suggested models, and common to all of them is that they rely on the Hebbian theory. In simple manners the theory says that if firing of neuron A is frequently followed by firing of neuron B, then the connection between neuron A and B will strengthen (\cite{Hebb}). The functional expressions that describe this synaptic plasticity is referred to as \textit{learning rules}. Classical learning rules for LTP and LTD, considers the instantaneous firing rates of the pre- and postsynaptic neurons. In the \textit{spike-timing-dependent plasticity} (STDP) learning rule, instead the change in connectivity depends on single spikes of pre- and postsynaptic neurons at short lags, inspired by \cite{Linderman}. This is the variant to be considered in this project, whose mathematical formulations is presented in section \ref{sec:LR}.

\subsection{STDP rules}
\label{sec:LR}

Let $F(\leq t)$ be a collection of measured neural firing up till time $t$ and let $C(t)$ be a measure of the synaptic strength between neuron A and neuron B at time $t$. The time unit for $t$ is not specified here, but is assumed to be suitable. The update of this connectivity from one time step to another can be expressed as

\begin{equation}
\label{eq:LR}
    C(t+1) = C(t+1) + l(C(t), F(\leq t)) + \epsilon,
\end{equation}

where $l$ is a learning rule depending on the current connectivity and the firing history and $\epsilon$ is some random noise. 

In the STDP-rule the synaptic connection between two neurons is updated according to the spike history of those two neurons only. Other rules includes dependencies across the whole network. Potentation occurs whenever neuron B spikes shortly after neuron A, and depression if neuron A spikes shortly after neuron B. The additive STDP rule looks like

\begin{equation}
\label{eq:STDP}
    \begin{split}
    l(C(t), F(\leq t)) = l_+(F(<t), A_+,\tau_+) - l_-(F(<t), A_-,\tau_-),\\ 
    l_+(F(\leq t), A_+,\tau_+) = F_B(t) \sum_{t'=0}^{t} F_A(t') A_+ e^{(t-t')/\tau_+},\\ 
    l_-(F(\leq t), A_-,\tau_-) = F_A(t) \sum_{t'=0}^{t} F_B(t') A_- e^{(t-t')/\tau_-},
    \end{split}
\end{equation}

where $F_A(t)$ is a binary variable equal to 1 if neuron A fires at time t and 0 otherwise. The parameters $\tau_+$ and $\tau_-$ control the scale of lags in which the firing contributes to connection updates. Decreasing the $\tau$ value corresponds to shrinking the window for where firing has significant impact on the plasticity. According to \cite{Song} experiments have shown that the value for $\tau_+$ is in the order of ten milliseconds. Some experiments suggest a value for $\tau_-$ of the same size, while other suggests a value somewhat larger. The parameters $A_+$ and $A_-$ scales the size of the updates, and corresponds to the maximum value of connectivity updates when $\abs (t-t')$ is small. For a "stable competitive synaptic modification", it is necessary that $A_-\tau_- > A_+\tau_+$, \cite{Song}. Therefore, if the two $\tau$-values are set equal, the value for $A_-$ has to be larger than $A_+$.

A slightly more complicated variant is the multiplicative STDP rule, which looks like

\begin{equation}
    l(C(t), S_{\leq t}) = \Tilde{l}_+(S_{<t}, A_+,\tau_+)(C_{max} - C(t))  - \Tilde{l}_-(S_{<t}, A_-,\tau_-)(C(t) - C_{min})
\end{equation}

where $\Tilde{l}_{\pm} = min(l_{\pm},1)$, and $C_{max}$ and $C_{min}$ are upper and lower bounds for the connections. 

\subsection{Linderman's work}
\label{Linderman}
As mentioned, the methods in this project is inspired by a paper, "A framework for studying synaptic plasticity with neural spike train data", by Scott W. Linderman, Christopher H. Stock and Ryan P. Adams from 2014 (about the same material is also found in Scott Linderman's PhD thesis form 2016). It is therefore relevant to summarize some of the methods and findings in the paper, to point out which ideas I have taken from it. Theoretical details are omitted.

Linderman models firing of single neurons as a Poisson process with time varying firing rate. This rate depends on a constant background rate, a linear function of some stimulus and the activity of connected neurons through unknown connectivity strengths. The connectivity is hypothesized to develop according to either of the two STDP rules presented in section \ref{sec:LR}, and their aim is to infer this from spike data. A method called particle MCMC is used for inference. 

They apply the inference method on GLM-based simulations and on data from the biophysical simulator NEURON. For the GLM variant they generate spikes for one synaptic connection with the additive STDP, the multiplicative STDP and for a static weight. They set the baseline firing rate to be normally distributed with a mean of 20 spikes per second, and update the connections every second for a total of 60 seconds. Then they fit four different models, where two are the STDP models, for the first 50 seconds of spiking data, and calculate the predictive log-likelihood of the models on the remaining 10 seconds. Their results show that for model corresponding to that used for generating data, the connections are inferred quite well compared to with the other models. In all cases the correct model scored highest in predictive log likelihood. This means that the method worked for distinguishing between the learning rules.













\cleardoublepage