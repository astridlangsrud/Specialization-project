%===================================== CHAP 3 =================================

\chapter{Background and theory}
\label{ch:theory}

The goal is to characterize the dynamics of synaptic connections based on the data material from the lab experiments. Since we know that simultaneous spiking of the presynaptic and postsynaptic neuron affects the synaptic connection, it is relevant to study the changing of neuronal spike rates in relation to the spiking of the surrounding neurons. For one neuron at one time step, the situation of spiking or not spiking can be considered as a Bernoulli process, with some time dependent spiking rate $\lambda (t)$. This spiking rate can be expressed function of the spike history of the neuron itself and of the other neurons, which can be modelled using a Generalized Linear Model (GLM). The relevant statistical theory is described in section \ref{sec:stats}. Section \ref{Intro_GLM} provides a general introduction to the GLM framework and presents how it looks like for a Bernoulli distribution. In section \ref{sec:Inference} it is explained how inference in GLMs can be made according to the maximum likelihood and gradient based methods. 

Gradient based methods with maximum likelihood works well if the problem is convex and enough information is stored in the data. If we have a system with stationary connectivity, this approach is suitable. For a network of N neurons with dynamical connections over T time steps, the number of parameters is of $O(N^2 \times T)$, whereas the number of data points is of $O(N \times T)$. This means that the system with dynamical connections is undersampled for the technique presented in section \ref{sec:Inference}. However, there are big correlations in time, and we have hypotheses on the connectivity dynamics from learning rules. Therefore, taking advantage of the prior knowledge with a Bayesian approach is a natural choice. A description of the Bayesian framework is given in section \ref{BayesianFW}. Including prior distributions typically do not preserve the convexity of the problem, so we have to resort to Monte-Carlo techniques to characterize the posterior. The Metropolis-Hastings algorithm is chosen as a starting point. The background theory for this is provided in section \ref{Metropolis}, where the information is gathered from the source \cite{MC}.

The hypothesized models for connectivity dynamics are based on observations from lab experiments with neural networks. Section \ref{sec:SP} presents background for some general learning rules, those that are investigated in Linderman's paper. In this project the learning rules will only be used for generating neural activity in simulations, and not for inference. For my master thesis inferring dynamical connectivity using learning rules will be a main focus. Therefore, this background is included here to give some context, and to enable a discussion of the results with reference to the learning rules.

\section{Generalized linear models}
\label{sec:stats}

\subsection{Introduction}
\label{Intro_GLM}
Consider the data set $ \{y_{i}, \bold{x_{i}}\}_{i=1}^{n} $ of $i$ sampling units, where $y_i$ represents an observed response value and $ \bold{x_i}$ are the corresponding explanatory variables. In general linear regression the relationship between the dependent and independent variables is modeled by the linear function:

\begin{equation}
\label{eq:general}
    \textbf{y} = \bm{ \beta}\bf X + \bm{ \epsilon}
\end{equation}

where \textbf{y} is a vector of response variables, \textbf{X} a matrix of explanatory variables, $\bm{\beta}$ is a vector of regression coefficients, and $\bm{ \epsilon}$ represents the error terms. The errors, $\epsilon_{i}$, are considered independent and identically distributed as $N(0, \sigma^{2})$.

Even though this model is useful for many situations, it has some limitations. For example, if the range of the $x$-values is $(-\infty, \infty)$, letting an $x$ approach infinity while everything else is kept constant makes also the corresponding $y$-value approach infinity (or minus infinity if $\beta$ is negative). Hence, if the range of $y$ should be restricted, the linear model is inappropriate. 

Generalized linear models extends the framework of the general linear models, by allowing the response variable to come from several other distributions than the normal one. The response variable can now be distributed according to some exponential family, which are distributions on the form

\begin{equation}
    f(y_i;\theta_i) = \text{exp}\Big(\frac{y_i \theta_i - b(\theta_i)}{\phi} \cdot w_i + c(y_i,\phi,w_i)\Big),
\end{equation}

where $b(\theta_i)$ and $c(y_i,\phi,w_i)$ are known functions, $\theta_i$ is the canonical parameter, $\phi$ is a nuisance parameter and $w_i$ is a weight function. The expected value of the distribution, $\mathbf{E}[y_i] = \mu_i$ is related to the canonical parameter by,

\begin{equation}
    \mu_i = b'(\theta_i).
\end{equation}

An essential property of the GLM framework is that there is a specified functional relationship, $g$, between the linear predictor $\eta_i = \bf x_i^T \bm{ \beta}$ and this mean value.

The GLM framework can be summarized by the following three components:

\begin{itemize}

\item Response variable distributed as some exponential family

\begin{equation}
    y_{i} \sim f(y_i;\theta_i)
\end{equation}

with expected value, $\mathbf{E}[y_i] = \mu_i$.

\item Linear predictor

\begin{equation}
    \eta_i = \bf x_i^T \bm{ \beta}
\end{equation}

\item Link function

\begin{equation}
    \eta_i = g(\mu_i)
\end{equation}

\end{itemize}



%Module pages from the GLM course (allowed as reference? Do I need reference here, or is this considered basic enough to not need a reference?)

%Often the link function used is the \textit{canonical link function}. This refers to the 

If the link function corresponds to functional relationship between the canonical parameter and the mean,

\begin{equation}
\theta_i = g(\mu_i),
\end{equation}

it is referred to as the \textit{canonical link function}. This link function is often chosen, as it comes with some advantageous properties.

The general linear model, given by equation \ref{eq:general}, is one special case of GLMs. It can be defined in the GLM framework, by specifying $y_i$ as a normal distributed variable with mean $\mu_i$, and having the identity link function. That is

\begin{equation}
\begin{split}
y \sim N(\mu_i, \sigma^2) \\
\mu_i = \eta_i = \bf x_i^T \bm{ \beta}
\end{split}
\end{equation}

\subsubsection{Bernoulli GLM}

In a Bernoulli process the response variable, $y_i$, takes the value 1 with a probability $\mu_i$, and 0 with the probability $1-\mu_i$. The corresponding probability density function is,

\begin{equation}
\begin{split}
    f(y_i|\mu_i) = Ber(\mu_i) = \mu_i^{y_i}(1-\mu_i)^{1-y_i}\\
    = \text{exp} ( y_i  \text{log}\big(\frac{\mu_i}{1-\mu_i}\big) + \text{log}(1-\mu_i)),
\end{split}
\end{equation}

where the bottom line shows that it corresponds to an exponential family. Hence, it can be modelled with GLM. The expected value of $y_i$ is given by

\begin{equation}
\mathbf{E} [y_i] = \sum_{y_i = 0,1} y_i \times \mu_i^{y_i}(1-\mu_i)^{1-y_i} = \mu_i
\end{equation}

A probability parameter can only take value in $[0,1]$. Thus the inverse of the link function, the \textit{response function}, have to be a mapping from the real line to $[0,1]$. The most common is the \textit{logit} link function, which is the canonical link in this case,

\begin{equation}
    \eta_i = g(\mu_i) = ln(\frac{\mu_i}{1-\mu_i}) \Leftrightarrow 
    \mu_i = h(\eta_i) = \frac{\exp(\eta_i)}{1+\exp(\eta_i)}
\end{equation}


\subsection{Maximum likelihood inference}
\label{sec:Inference}

Typically the $\bm {\beta}$-values in the linear predictor are unknown, and the goal is to estimate these. Given the input and response variables, one wants to find the $\bm {\beta}$-values that fits best to this data. This is done by searching for the values that makes the observed data most likely, namely those that maximizes the likelihood:

\begin{equation}
    L(\bm{\beta}) = \prod_{i=1}^{n} f(y_i |\bm{ \beta})
\end{equation}

For a Bernoulli distribution this is,

\begin{equation}
    L(\bm{\beta}) = \prod_{i=1}^{n} f(y_i |\bm{ \beta}) = \prod_{i=1}^{n} \mu_i^{y_i}(1-\mu_i)^{1-y_i}
\end{equation}

The logarithm of the likelihood has the same maximizing $\bm{\beta}$-values, and is often easier to operate with. The loglikelihood is derived as,

\begin{equation}
\begin{split}
    l(\bm{\beta}) = \log \prod_{i=1}^{n} (\mu_i^{y_i}(1-\mu_i)^{1-y_i})  =
    \sum_{i=1}^n log (\mu_i^{y_i}(1-\mu_i)^{1-y_i})\\ =  \sum_{i=1}^{n} y_i \log(\frac{\mu_i}{1-\mu_i}) + \log(1-\mu_i)
\end{split}
\end{equation}

Then, substituting in $\mu_i = \frac{exp(\eta_i)}{1+exp(\eta_i)}$, one arrives at

\begin{equation}
    l(\bm{\beta}) = \sum_{i=1}^{n} y_i \eta_i - log(1 + exp(\eta_i) = \sum_{i=1}^n y_i{\bf x_i^T} \bm{ \beta} - ln(1 + exp(\bf x_i^T \bm{ \beta}))
\end{equation}

The goal is to find the parameters that maximizes the likelihood. For a convex problem inference can be done using gradient based iterative methods. The idea of such optimization algorithms is to searches in the parameter space in the direction of positive gradient to arrive at a maximum. One famous such method is the Newton method. Generally, for a function $f(x)$, one starts by choosing some initial guess $x^{(0)}$, and hence update the approximation in every iteration by

\begin{equation}
    x^{(i+1)} = x^{(i)} - \frac{f'(x^{(i)})}{f''(x^{(i)})}
\end{equation}

The first and second derivative of a loglikelihood function is called the score function and the observed Fisher information matrix.

The score function is the vector of partial derivatives of the loglikelihood. In the Bernoulli case the score function can be derived as follows

\begin{equation}
\label{eq:score}
s(\bm{\beta}) = \sum_{i=1}^{n}s_i(\bm{\beta}) = \sum_{i=1}^{n} \frac{\partial l_i(\bm{\beta})}{\partial \bm{\beta}} = \sum_{i=1}^{n} {\bf x_i} \Big ( y_i - \frac{exp({\bf x_i^T} \bm{ \beta})}{1 + exp({\bf x_i^T} \bm{ \beta})}\Big )
\end{equation}

The observed Fisher information matrix is defined as 

\begin{equation}
    H(\bm {\beta}) = - \frac{\partial^2 l(\bm {\beta)}}{\partial \bm{\beta} \partial \bm{\beta}^T} = -\frac{\partial s(\bm{\beta})}{\partial \bm{\beta}^T},
\end{equation}

which for the Bernoulli case corresponds to

\begin{equation}
    H(\bm{\beta}) = \sum_{i=1}^{n} {\bf x_i x_i^T} \pi_i (1-\pi_i)
\end{equation}

Hence, the Newton method for estimating $\beta$, is the iteration scheme

\begin{equation}
\label{eq:Newton}
    \bm{\beta}^{(i+1)} = \bm{\beta}^{(i)} + (H(\bm{\beta}^{(i)}))^{-1} s(\bm{\beta}^{(i)}
\end{equation},

where $H(\bm{\beta}^{(i)})^{-1}$ is the matrix inverse of the observed Fisher information matrix.



\section{Bayesian framework}
\label{BayesianFW}

Given some data sample ${\bf x}$ from a parametric distribution $P(x;\theta)$ where the parameter, $\theta$, is unknown, the posterior distribution of the parameter given the data can be expressed according to Bayes theorem, as follows.

\begin{equation}
    P(\theta | {\bf x}) = \frac{P({ \bf x}|\theta)P(\theta)}{P({\bf x})}
\end{equation}

$P({\bf x})$ is the likelihood that the data would be observed given $\theta$. $P(\theta)$ is the prior distribution of $\theta$, and expresses some knowledge one has of $\theta$ before seeing the data. $P({\bf x}$ is the marginal distribution of the data, and can be expressed as,

\begin{equation}
    P({\bf x}) = \int P({\bf x},\theta)d\theta.
\end{equation}

This is typically too complicated to solve, and can make it is impossible to sample directly from the posterior distribution. However, given the data $x$, $P(x)$ is a constant value, so the posterior distribution is proportional to the numerator, $P(x|\theta)P(\theta)$. 

\begin{equation}
    P(\theta | {\bf x}) \propto P({\bf x}|\theta)P(\theta)
\end{equation}

The relative amount that the likelihood and the prior contributes to the posterior depends on their variances and the size of the data sample. If the prior knowledge is that the parameter exists in some narrow window, and this prior knowledge is very certain, the variance in the prior distribution can be set very low. This makes the prior more dominating than if its variance was higher. As the size of the data set grows, the relative contribution of the likelihood function increases. 






%In the gradient ascent method one begins with some initial guess $\beta = \beta^0$, and iterates towards the solution by the direction of the gradient of $\beta^i$, and updating the parameters to move in that direction by some step length $\gamma$.

%\begin{equation}
    %\beta^{i+1} = \beta^{i} + \gamma s(\beta)
%\end{equation}

%For a suitable problem this will converge, and eventually the parameters will have a value sufficiently close to that of maximum likelihood.

%Let $\{t_0, t_1, ..., t_n\}$ be a sequence of equally spaced time steps in [0,T] and let $s_i(t_j) \in \{0,1\}$ be the random event taking value 1 if $N_i$ spikes in the time interval $(t_{j-1}, t_{j})$, and 0 otherwise. If the time intervals are so small that the probability of covering more then one spike is negligible, each time interval can be regarded as a Bernoulli process with some probability, $p$, of spiking. This $p$ can be dependent on for instance the spike history of the other neurons, and the weight matrix.  

\section{Inference with MCMC}
\label{Bayesian}

A Markov chain Monte Carlo (MCMC) techniques utilizes, as the name suggests, a combination of Markov chains with Monte Carlo sampling. A Markov chain is a sequence of events, typically time indexed, that satisfies the Markov property, which says that future events only depends on the present state and not on the past. Monte Carlo techniques is the use of random sampling from a distribution to make numerical estimates. However, this requires that we can sample from the distribution, which is not always straight forward. So the idea of MCMC to construct a Markov chain that has a limiting distribution equal to the one we want to sample from, and this way obtain the sample we are looking for. Metropolis-Hastings is one of several algorithms that does this.

\subsection{Metropolis-Hastings algorithm}
\label{Metropolis}

Assume we want to sample from a target distribution $p(x)$, which cannot be sampled from directly. Also assume that $Q(x|x')$ is a parametric distribution that is possible to draw direct samples from and that satisfies the following two properties

\begin{enumerate}
    \item $Q(x|x') = Q(x'|x)$
    \item $Q(x|x')>0,   \hspace{4mm} \forall \; (x; p(x)>0), \; (x';(p(x')>0)$
\end{enumerate}

Then the Metropolis-Hastings procedure for sampling form $p(x)$ by using $Q$ as proposal distribution is summarized in algorithm \ref{alg:M-H1}.

\begin{algorithm}
\caption{}\label{alg:M-H1}
\begin{algorithmic}
\State Set starting value $x^0$
\For {$i=0,1,2,\ldots$}
\State Draw x' from $Q(x'|x^i)$
\State $x^{i+1} = \begin{cases} x' \text{ with probability } min\{ 1, \frac{p(x')}{p(x^i)} \}\\ x^i \text{ with probability } 1-min\{ 1, \frac{p(x')}{p(x^i)} \}
\end{cases}$
\EndFor
\end{algorithmic}
\end{algorithm}

The resulting sequence $\{  x^0, x^1, x^2,... \}$ is then a Markov chain with transition probability

\begin{equation}
    T(x^{i+1} = x|x^i) = min \Big \{ 1, \frac{p(x)}{p(x^i)} \Big \} Q(x|x^i).
\end{equation}

It can be shown that the reversibility condition, 

\begin{equation}
    p(x)T(x'|x) = p(x')T(x|x'),
\end{equation} 

is satisfied, which means that $p(x)$ is a stationary distribution of the Markov chain. Hence, for some index $n$, we have that $\{x_n, x_{n+1}, x_{n+2},...\}$ is an approximate sample from $p(x)$.

 %This can be done if we cannot sample from a distribution directly. This means that in collecting sample values $\{\theta_0, \theta_1, \theta_2,...\}$ for an unknown parameter $\theta$, the sample $\{\theta_m, \theta_{m+1}, \theta_{m+2},...\}$ will approximately correspond to a random sample from the posterior distribution for $m$ large enough. 

%The Metropolis-Hastings algorithm is initiated by guessing some first value of the parameter, $\theta_0$, for the sample. In every iteration a new value $\theta_{new}$ is drawn from a proposal distribution, $Q(\theta_{new}|\theta_i)$, that is dependent on the previous accepted $\theta$-value for the sample, $\theta_i$. Q can for example be a gaussian distribution with mean value equal to $\theta_i$. This is the Markov chain part of the algorithm. The next step is to determine if $\theta_{new}$ is accepted for the sample or not. This is done by computing the ratio

%\begin{equation}
    %\alpha = 
    %\frac{P(\theta_{new}|{\bm x})}{P(\theta_i|{\bm x})} = 
    %\frac{P({\bm x}|\theta_{new})P(\theta_{new})}{P({\bm x}|\theta_{i})P(\theta_{i})}
%\end{equation}

%If this ratio is greater than or equal to one, the suggestion $\theta_{new}$ is directly accepted. If the ratio is less than one, the suggestion is accepted with a probability equal to the ratio. If $\theta_{new}$ is accepted, we set $\theta_{i+1} = \theta_{new}$, and if not we set $\theta_{i+1} = \theta_i$.




\section{Modelling synaptic plasticity}
\label{sec:SP}

Decades of experimental research have revealed certain relations for how spiking of neurons affects the strength of connectivity between them. There are various suggested models, and common to all of them is that they rely on the Hebbian theory. In simple manners the theory says that if firing of neuron A frequently induce firing of neuron B, then the connection between the two neurons will strengthen (\cite{Hebb}). The functional expressions that describe this synaptic plasticity is referred to as \textit{learning rules}. Classical learning rules for LTP and LTD, considers the instantaneous firing rates of the pre- and postsynaptic neurons. The somewhat newer model described in Linderman, the \textit{spike-timing-dependent plasticity} (STDP) learning rule, takes a different approach. It examine individual spikes and involves a consideration of the time interval between firing of the two neurons. This is the variant to be considered in this project, and the matemathical formulations is presented in section \ref{sec:LR}.

\subsection{Learning rules}
\label{sec:LR}

Let $F(\leq t)$ be a collection of measured neural firing up till time $t$ and let $C(t)$ be a measure of the synaptic strength between neuron A and neuron B at time $t$. The time unit for $t$ is not specified here, but is assumed to be suitable. The update of this connectivity from one time step to another can be expressed as

\begin{equation}
\label{eq:LR}
    C(t+1) = C(t+1) + l(C(t), F(\leq t)) + \epsilon,
\end{equation}

where $l$ is a learning rule depending on the current connectivity and the firing history and $\epsilon$ is some random noise. 

In the STDP-rule the synaptic connection between two neurons is updated according to the spike history of those two neurons only. Other rules includes dependencies across the whole network. Potentation occurs whenever neuron B spikes shortly after neuron A, and depression if neuron A spikes shortly after neuron B. The canonical STDP rule looks like

\begin{equation}
\label{eq:STDP}
    \begin{split}
    l(C(t), F(\leq)) = l_+(F(<t), A_+,\tau_+) - l_-(F(<t), A_-,\tau_-),\\ 
    l_+(F(\leq t), A_+,\tau_+) = F_B(t) \sum_{t'=0}^{t} F_A(t') A_+ e^{(t-t')/\tau_+},\\ 
    l_-(F(\leq t), A_-,\tau_-) = F_A(t) \sum_{t'=0}^{t} F_B(t') A_- e^{(t-t')/\tau_-},
    \end{split}
\end{equation}

where $F_A(t)$ is a binary variable equal to 1 if neuron A fires at time t and 0 otherwise. The parameters $\tau_+$ and $\tau_-$ affects the rage of time intervals in which the firing contributes to connection updates. Decreasing the $\tau$ value corresponds to shrinking the window for where firing has significant impact on the plasticity. According to \cite{Song} experiments have shown that the value for $\tau_+$ is in the order of ten milliseconds. Some experiments suggest a value for $\tau_-$ of the same size, while other suggests a value somewhat larger. The parameters $A_+$ and $A_-$ scales the size of the updates, and corresponds to the maximum value of connectivity updates when $\Delta t$ is small. For a "stable competitive synaptic modification", it is necessary that $A_-\tau_- > A_+\tau_+$, \cite{Song}. Therefore, if the two $\tau$-values are set equal, the value for $A_-$ has to be larger than $A_+$.

A slightly more complicated variant is the multiplicative STDP rule, which looks like

\begin{equation}
    l(C(t), S_{\leq t}) = \Tilde{l}_+(S_{<t}, A_+,\tau_+)(C_{max} - C(t))  - \Tilde{l}_-(S_{<t}, A_-,\tau_-)(C(t) - C_{min})
\end{equation}

where $\Tilde{l}_{\pm} = min(l_{\pm},1)$, and $C_{max}$ and $C_{min}$ are upper and lower bounds for the connections. 

\subsection{Linderman's work}
\label{Linderman}
As mentioned, the methods in this project is inspired by a paper, "A framework for studying synaptic plasticity with neural spike train data", by Scott W. Linderman, Christopher H. Stock and Ryan P. Adams from 2014 (about the same material is also found in Scott Linderman's PhD thesis form 2016). It is therefore relevant to present some of the material and results from the paper, to point out which ideas I have taken from it. 













\cleardoublepage